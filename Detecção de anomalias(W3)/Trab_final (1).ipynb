{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3WOdNszFM25",
        "outputId": "a0ca7a03-ebbd-4df3-d7e5-320eba84447b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fastparquet in /usr/local/lib/python3.11/dist-packages (2024.11.0)\n",
            "Requirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from fastparquet) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fastparquet) (1.26.4)\n",
            "Requirement already satisfied: cramjam>=2.3 in /usr/local/lib/python3.11/dist-packages (from fastparquet) (2.9.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from fastparquet) (2024.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from fastparquet) (24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0->fastparquet) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0->fastparquet) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0->fastparquet) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install fastparquet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-i38lHdYApGs"
      },
      "source": [
        "Sem Pyspark:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dh2-uU2oArVP",
        "outputId": "debb666c-4f83-4829-bbf2-f3555c049479"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iniciando o carregamento e pré-processamento dos dados...\n",
            "Tempo de carregamento e pré-processamento: 41.04 segundos\n",
            "Iniciando o treinamento do modelo Random Forest...\n",
            "Tempo de treinamento e avaliação: 7456.80 segundos\n",
            "Tempo total de execução: 7497.99 segundos\n",
            "Acurácia Balanceada: 0.8709\n",
            "Macro F1-Score: 0.8716\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import time\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import balanced_accuracy_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Função para carregar e pré-processar os dados\n",
        "def load_and_preprocess_data(file_path):\n",
        "    print(\"Iniciando o carregamento e pré-processamento dos dados...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Carregar o arquivo Parquet consolidado\n",
        "    data = pd.read_parquet(file_path, engine=\"fastparquet\")\n",
        "\n",
        "    # Filtrar classes relevantes (0 a 9)\n",
        "    valid_classes = list(range(10))\n",
        "    if 'class' in data.columns:\n",
        "        data = data[data['class'].isin(valid_classes)]\n",
        "\n",
        "    # Separar features e rótulos\n",
        "    X = data.drop(columns=['class', 'state']).dropna()\n",
        "    y = data.loc[X.index, 'class']\n",
        "\n",
        "    # Normalizar as features\n",
        "    scaler = MinMaxScaler()\n",
        "    X_normalized = scaler.fit_transform(X)\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"Tempo de carregamento e pré-processamento: {elapsed_time:.2f} segundos\")\n",
        "    return X_normalized, y\n",
        "\n",
        "# Função para treinar o modelo Random Forest\n",
        "def train_random_forest(X, y):\n",
        "    print(\"Iniciando o treinamento do modelo Random Forest...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Dividir os dados em treino e teste\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "    # Definir o modelo Random Forest com hiperparâmetros ajustados\n",
        "    model = RandomForestClassifier(\n",
        "        n_estimators=100,  # Número de árvores na floresta\n",
        "        max_depth=10,      # Profundidade máxima das árvores\n",
        "        random_state=42,   # Reprodutibilidade\n",
        "        n_jobs=-1          # Usar todos os núcleos disponíveis\n",
        "    )\n",
        "\n",
        "    # Treinar o modelo\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Fazer previsões no conjunto de teste\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Avaliar o modelo\n",
        "    balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
        "    macro_f1_score = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"Tempo de treinamento e avaliação: {elapsed_time:.2f} segundos\")\n",
        "    return model, balanced_accuracy, macro_f1_score\n",
        "\n",
        "# Medir tempo total de execução do script\n",
        "overall_start_time = time.time()\n",
        "\n",
        "# Caminho do arquivo consolidado\n",
        "input_file = \"/content/dados_limpos1.parquet\"\n",
        "\n",
        "# Carregar e pré-processar os dados\n",
        "X, y = load_and_preprocess_data(input_file)\n",
        "\n",
        "# Treinar e avaliar o modelo\n",
        "model, balanced_accuracy, macro_f1_score = train_random_forest(X, y)\n",
        "\n",
        "# Tempo total do script\n",
        "overall_elapsed_time = time.time() - overall_start_time\n",
        "\n",
        "# Exibir métricas finais\n",
        "print(f\"Tempo total de execução: {overall_elapsed_time:.2f} segundos\")\n",
        "print(f\"Acurácia Balanceada: {balanced_accuracy:.4f}\")\n",
        "print(f\"Macro F1-Score: {macro_f1_score:.4f}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3ViKrYWAfzw"
      },
      "source": [
        "Com Pyspark:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mfXhi5uVUwD",
        "outputId": "5c621061-ea8a-47a1-fed5-b5716b342219"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.4)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3MUfMYHVOH0",
        "outputId": "60e0dedb-8220-4207-8bd8-16b579b08b99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iniciando o carregamento e pré-processamento dos dados...\n",
            "Carregamento e pré-processamento concluídos. Tempo: 369.47 segundos\n",
            "Iniciando o treinamento do modelo Random Forest...\n",
            "Treinamento e avaliação concluídos. Tempo: 10455.26 segundos\n",
            "Acurácia Balanceada: 0.9933\n",
            "Macro F1-Score: 0.9928\n",
            "\n",
            "Resumo de execução:\n",
            "Tempo de carregamento e pré-processamento: 369.47 segundos\n",
            "Tempo de treinamento e avaliação: 10455.26 segundos\n",
            "Tempo total de execução: 10824.74 segundos\n",
            "Acurácia Balanceada: 0.9933\n",
            "Macro F1-Score: 0.9928\n",
            "Execução concluída.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import time\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler, MinMaxScaler\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Configurar a sessão do Spark para o Colab\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "    .appName(\"PySparkRandomForestColab\")\n",
        "    .config(\"spark.executor.memory\", \"11g\")  # Ajustar memória do executor\n",
        "    .config(\"spark.driver.memory\", \"8g\")  # Ajustar memória do driver\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"12\")  # Ajustar partições\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "# Função para carregar e pré-processar os dados\n",
        "def load_and_preprocess_data(file_path):\n",
        "    start_time = time.time()\n",
        "    print(\"Iniciando o carregamento e pré-processamento dos dados...\")\n",
        "\n",
        "    # Carregar os dados usando PySpark\n",
        "    data = spark.read.parquet(file_path)\n",
        "\n",
        "    # Filtrar classes relevantes\n",
        "    data = data.filter(data[\"class\"].isin([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))\n",
        "\n",
        "    # Montar o vetor de features\n",
        "    feature_columns = [col for col in data.columns if col not in [\"class\", \"state\"]]\n",
        "    assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "    data = assembler.transform(data)\n",
        "\n",
        "    # Normalizar as features\n",
        "    scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
        "    scaler_model = scaler.fit(data)\n",
        "    data = scaler_model.transform(data)\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"Carregamento e pré-processamento concluídos. Tempo: {elapsed_time:.2f} segundos\")\n",
        "    return data, elapsed_time\n",
        "\n",
        "# Função para treinar o modelo Random Forest\n",
        "def train_random_forest(data):\n",
        "    start_time = time.time()\n",
        "    print(\"Iniciando o treinamento do modelo Random Forest...\")\n",
        "\n",
        "    # Dividir os dados em treino e teste\n",
        "    train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "    # Treinar o modelo Random Forest\n",
        "    rf = RandomForestClassifier(labelCol=\"class\", featuresCol=\"scaledFeatures\", numTrees=100, maxDepth=10)\n",
        "    model = rf.fit(train_data)\n",
        "\n",
        "    # Fazer previsões no conjunto de teste\n",
        "    predictions = model.transform(test_data)\n",
        "\n",
        "    # Avaliar o modelo\n",
        "    evaluator = MulticlassClassificationEvaluator(labelCol=\"class\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
        "    balanced_accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "    evaluator_macro_f1 = MulticlassClassificationEvaluator(labelCol=\"class\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "    macro_f1_score = evaluator_macro_f1.evaluate(predictions)\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"Treinamento e avaliação concluídos. Tempo: {elapsed_time:.2f} segundos\")\n",
        "    print(f\"Acurácia Balanceada: {balanced_accuracy:.4f}\")\n",
        "    print(f\"Macro F1-Score: {macro_f1_score:.4f}\")\n",
        "    return model, elapsed_time, balanced_accuracy, macro_f1_score\n",
        "\n",
        "# Medir tempo total\n",
        "overall_start_time = time.time()\n",
        "\n",
        "# Caminho do arquivo de entrada\n",
        "input_file = \"/content/dados_limpos1.parquet\"\n",
        "\n",
        "# Etapa 1: Carregar e pré-processar os dados\n",
        "data, preprocessing_time = load_and_preprocess_data(input_file)\n",
        "\n",
        "# Etapa 2: Treinar e avaliar o modelo\n",
        "model, training_time, balanced_accuracy, macro_f1_score = train_random_forest(data)\n",
        "\n",
        "# Exibir tempo total de execução\n",
        "overall_elapsed_time = time.time() - overall_start_time\n",
        "print(\"\\nResumo de execução:\")\n",
        "print(f\"Tempo de carregamento e pré-processamento: {preprocessing_time:.2f} segundos\")\n",
        "print(f\"Tempo de treinamento e avaliação: {training_time:.2f} segundos\")\n",
        "print(f\"Tempo total de execução: {overall_elapsed_time:.2f} segundos\")\n",
        "print(f\"Acurácia Balanceada: {balanced_accuracy:.4f}\")\n",
        "print(f\"Macro F1-Score: {macro_f1_score:.4f}\")\n",
        "\n",
        "# Finalizar o Spark\n",
        "spark.stop()\n",
        "print(\"Execução concluída.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBUUes90fpRB",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "Conclusões finais:\n",
        "\n",
        "O uso do PySpark resultou em um modelo significativamente mais preciso, com ganhos de 14,05% na acurácia balanceada e 13,90% no Macro F1-Score. Isso provavelmente se deve ao paralelismo interno do RandomForestClassifier no Spark, que ajuda a capturar padrões mais robustos nos dados.\n",
        "\n",
        "\n",
        "Também podemos notar aumentou o tempo de execução em aproximadamente 44% com o uso do PySpark, provavelmente devido ao overhead de configuração e execução distribuída no Spark. Para datasets pequenos ou médios, o PySpark pode ser menos eficiente em relação à execução direta.\n",
        "\n",
        "\n",
        "Para o caso estudado aqui podemos notar um claro trade-off: maior desempenho em clasificação ao custo de um tempo de execução maior.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
