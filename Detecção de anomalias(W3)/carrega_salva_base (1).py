# -*- coding: utf-8 -*-
"""carrega_salva_base.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18fvEkUl5R3Tc9YkFuSbAGxc4OP_Fc06a
"""

import os
import pandas as pd
import requests

def fetch_parquet_files_from_github(repo_url, local_folder):
    """
    Função para buscar, baixar e consolidar arquivos Parquet de um repositório GitHub.

    Parâmetros:
        repo_url (str): URL do repositório GitHub.
        local_folder (str): Caminho local para salvar os arquivos baixados.

    Retorna:
        pd.DataFrame: DataFrame consolidado com os dados dos arquivos Parquet.
    """
    # Criar a pasta local se ela não existir
    if not os.path.exists(local_folder):
        os.makedirs(local_folder)

    # Configurar cabeçalhos da API do GitHub
    headers = {"Accept": "application/vnd.github.v3+json"}

    # Construir o URL da API para acessar os arquivos no repositório
    api_url = repo_url.replace("github.com", "api.github.com/repos") + "/contents/dataset"

    # Fazer uma requisição para listar o conteúdo da pasta `dataset`
    response = requests.get(api_url, headers=headers)
    if response.status_code != 200:
        raise Exception(f"Erro ao acessar {api_url}: {response.status_code}")

    parquet_files = []  # Lista para armazenar os caminhos dos arquivos Parquet baixados

    # Iterar sobre os itens retornados pela API (arquivos/pastas na pasta `dataset`)
    for folder_info in response.json():
        # Verificar se o item é uma pasta e se seu nome é um número (ex.: "123")
        if folder_info['type'] == 'dir' and folder_info['name'].isdigit():
            folder_url = folder_info['url']  # URL para acessar o conteúdo da subpasta
            folder_response = requests.get(folder_url, headers=headers)
            if folder_response.status_code != 200:
                print(f"Erro ao acessar a pasta {folder_info['name']}: {folder_response.status_code}")
                continue

            # Iterar sobre os arquivos na subpasta
            for file_info in folder_response.json():
                # Verificar se o arquivo tem extensão `.parquet`
                if file_info['name'].endswith('.parquet'):
                    file_url = file_info['download_url']  # URL para baixar o arquivo
                    local_file = os.path.join(local_folder, file_info['name'])  # Caminho local para salvar

                    # Baixar o arquivo Parquet e salvá-lo localmente
                    with open(local_file, 'wb') as f:
                        f.write(requests.get(file_url).content)

                    # Adicionar o caminho local à lista
                    parquet_files.append(local_file)

    # Verificar se nenhum arquivo Parquet foi encontrado
    if not parquet_files:
        raise ValueError("Nenhum arquivo Parquet encontrado nas pastas especificadas no repositório.")

    # Consolidar os dados de todos os arquivos Parquet em um único DataFrame
    all_dataframes = []
    for parquet_file in parquet_files:
        try:
            # Ler cada arquivo Parquet como um DataFrame
            df = pd.read_parquet(parquet_file, engine='fastparquet')
            all_dataframes.append(df)
        except Exception as e:
            print(f"Erro ao ler o arquivo {parquet_file}: {e}")

    # Verificar se nenhum DataFrame válido foi gerado
    if not all_dataframes:
        raise ValueError("Nenhum dataframe válido foi gerado a partir dos arquivos Parquet.")

    # Concatenar todos os DataFrames em um único DataFrame
    consolidated_data = pd.concat(all_dataframes, ignore_index=True)

    return consolidated_data  # Retornar o DataFrame consolidado

# URL base do repositório GitHub e o caminho local para salvar os arquivos
repo_url = "https://github.com/petrobras/3W"  # URL do repositório
local_folder = "dataset"  # Pasta local para salvar os arquivos

# Consolidar o dataset usando a API do GitHub
data_pandas = fetch_parquet_files_from_github(repo_url, local_folder)

# Exibir informações gerais do dataset consolidado
print(f"Total de registros consolidados: {len(data_pandas)}")  # Mostrar o total de registros
print("Exemplo de dados:")
print(data_pandas.head())  # Mostrar as primeiras linhas do DataFrame

# Salvar o DataFrame consolidado em um arquivo Parquet
output_file = "dados_consolidados.parquet"  # Nome do arquivo Parquet consolidado
data_pandas.to_parquet(output_file, engine="fastparquet")  # Salvar o DataFrame consolidado
print(f"Arquivo consolidado salvo em: {output_file}")  # Confirmar o salvamento
